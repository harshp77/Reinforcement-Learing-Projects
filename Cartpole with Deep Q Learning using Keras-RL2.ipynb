{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73db4fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc284127",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27920e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04065783,  0.001136  ,  0.01273832, -0.031902  ], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "731c7cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_actions = env.action_space.n\n",
    "nb_obs = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf0395cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(1,)+nb_obs))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6639be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 690\n",
      "Trainable params: 690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43c60939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c909ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit = 20000,window_length = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f761f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.policy import LinearAnnealedPolicy,EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "059e0b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),attr='eps',value_max=1.0,value_min=0.1,value_test=0.05,nb_steps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d430c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model,nb_actions=nb_actions,memory=memory,nb_steps_warmup=10,target_model_update=100,policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93c4bb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.compile(Adam(learning_rate=1e-3),metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0f0c63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 10 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 11 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 13 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 14 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 15 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 16 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 17 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 18 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 19 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 20 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 21 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 22 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 23 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 24 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 25 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 26 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 27 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 28 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 29 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 30 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\harsh\\AppData\\Roaming\\Python\\Python39\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    65/20000: episode: 1, duration: 0.732s, episode steps:  65, steps per second:  89, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 0.392155, mae: 0.504465, mean_q: 0.170860, mean_eps: 0.998313\n",
      "    92/20000: episode: 2, duration: 0.182s, episode steps:  27, steps per second: 148, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  loss: 0.161734, mae: 0.502069, mean_q: 0.567746, mean_eps: 0.996490\n",
      "   114/20000: episode: 3, duration: 0.148s, episode steps:  22, steps per second: 148, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.342728, mae: 0.779947, mean_q: 0.956096, mean_eps: 0.995387\n",
      "   160/20000: episode: 4, duration: 0.386s, episode steps:  46, steps per second: 119, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.130413, mae: 1.004086, mean_q: 1.680221, mean_eps: 0.993858\n",
      "   189/20000: episode: 5, duration: 0.139s, episode steps:  29, steps per second: 208, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 0.034728, mae: 1.004655, mean_q: 1.933060, mean_eps: 0.992170\n",
      "   200/20000: episode: 6, duration: 0.053s, episode steps:  11, steps per second: 207, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.017775, mae: 1.008353, mean_q: 1.966527, mean_eps: 0.991270\n",
      "   235/20000: episode: 7, duration: 0.195s, episode steps:  35, steps per second: 179, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.278515, mae: 1.564261, mean_q: 2.693063, mean_eps: 0.990235\n",
      "   251/20000: episode: 8, duration: 0.080s, episode steps:  16, steps per second: 201, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.100603, mae: 1.600396, mean_q: 3.210972, mean_eps: 0.989088\n",
      "   269/20000: episode: 9, duration: 0.084s, episode steps:  18, steps per second: 214, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.106476, mae: 1.583769, mean_q: 3.142182, mean_eps: 0.988323\n",
      "   280/20000: episode: 10, duration: 0.060s, episode steps:  11, steps per second: 184, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.076021, mae: 1.573455, mean_q: 3.163121, mean_eps: 0.987670\n",
      "   299/20000: episode: 11, duration: 0.103s, episode steps:  19, steps per second: 185, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.151636, mae: 1.594576, mean_q: 3.128515, mean_eps: 0.986995\n",
      "   324/20000: episode: 12, duration: 0.135s, episode steps:  25, steps per second: 185, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 0.408948, mae: 2.068697, mean_q: 3.662815, mean_eps: 0.986005\n",
      "   332/20000: episode: 13, duration: 0.043s, episode steps:   8, steps per second: 188, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.251287, mae: 2.269544, mean_q: 4.477598, mean_eps: 0.985262\n",
      "   345/20000: episode: 14, duration: 0.066s, episode steps:  13, steps per second: 196, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.276321, mae: 2.163454, mean_q: 4.206162, mean_eps: 0.984790\n",
      "   360/20000: episode: 15, duration: 0.080s, episode steps:  15, steps per second: 188, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.282995, mae: 2.143133, mean_q: 4.115323, mean_eps: 0.984160\n",
      "   381/20000: episode: 16, duration: 0.102s, episode steps:  21, steps per second: 206, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 0.163771, mae: 2.124876, mean_q: 4.270491, mean_eps: 0.983350\n",
      "   425/20000: episode: 17, duration: 0.202s, episode steps:  44, steps per second: 218, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 0.381565, mae: 2.447636, mean_q: 4.664901, mean_eps: 0.981887\n",
      "   469/20000: episode: 18, duration: 0.219s, episode steps:  44, steps per second: 201, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 0.395224, mae: 2.734200, mean_q: 5.300033, mean_eps: 0.979908\n",
      "   480/20000: episode: 19, duration: 0.101s, episode steps:  11, steps per second: 109, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.437504, mae: 2.736763, mean_q: 5.314035, mean_eps: 0.978670\n",
      "   497/20000: episode: 20, duration: 0.158s, episode steps:  17, steps per second: 108, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.297166, mae: 2.701472, mean_q: 5.318045, mean_eps: 0.978040\n",
      "   513/20000: episode: 21, duration: 0.158s, episode steps:  16, steps per second: 101, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.762946, mae: 3.107184, mean_q: 5.537218, mean_eps: 0.977297\n",
      "   525/20000: episode: 22, duration: 0.149s, episode steps:  12, steps per second:  80, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.582721, mae: 3.324596, mean_q: 6.511961, mean_eps: 0.976667\n",
      "   539/20000: episode: 23, duration: 0.139s, episode steps:  14, steps per second: 101, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.589872, mae: 3.278988, mean_q: 6.392542, mean_eps: 0.976083\n",
      "   551/20000: episode: 24, duration: 0.142s, episode steps:  12, steps per second:  84, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.613621, mae: 3.257859, mean_q: 6.222121, mean_eps: 0.975497\n",
      "   583/20000: episode: 25, duration: 0.249s, episode steps:  32, steps per second: 128, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.399194, mae: 3.266543, mean_q: 6.377843, mean_eps: 0.974507\n",
      "   611/20000: episode: 26, duration: 0.138s, episode steps:  28, steps per second: 202, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.784841, mae: 3.431407, mean_q: 6.382696, mean_eps: 0.973157\n",
      "   644/20000: episode: 27, duration: 0.153s, episode steps:  33, steps per second: 215, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.717079, mae: 3.785030, mean_q: 7.287537, mean_eps: 0.971785\n",
      "   672/20000: episode: 28, duration: 0.139s, episode steps:  28, steps per second: 201, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 0.633193, mae: 3.720790, mean_q: 7.193475, mean_eps: 0.970413\n",
      "   687/20000: episode: 29, duration: 0.076s, episode steps:  15, steps per second: 198, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 0.460433, mae: 3.689509, mean_q: 7.151159, mean_eps: 0.969445\n",
      "   710/20000: episode: 30, duration: 0.129s, episode steps:  23, steps per second: 179, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.753574, mae: 3.923269, mean_q: 7.396098, mean_eps: 0.968590\n",
      "   727/20000: episode: 31, duration: 0.092s, episode steps:  17, steps per second: 184, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.668326, mae: 4.252231, mean_q: 8.297302, mean_eps: 0.967690\n",
      "   750/20000: episode: 32, duration: 0.108s, episode steps:  23, steps per second: 213, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.133305, mae: 4.228157, mean_q: 7.958033, mean_eps: 0.966790\n",
      "   769/20000: episode: 33, duration: 0.085s, episode steps:  19, steps per second: 223, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.684 [0.000, 1.000],  loss: 1.005787, mae: 4.250501, mean_q: 8.105971, mean_eps: 0.965845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   802/20000: episode: 34, duration: 0.165s, episode steps:  33, steps per second: 200, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.394 [0.000, 1.000],  loss: 0.843919, mae: 4.231310, mean_q: 8.080342, mean_eps: 0.964675\n",
      "   814/20000: episode: 35, duration: 0.070s, episode steps:  12, steps per second: 171, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.705325, mae: 4.536696, mean_q: 8.633029, mean_eps: 0.963662\n",
      "   828/20000: episode: 36, duration: 0.066s, episode steps:  14, steps per second: 213, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 1.100514, mae: 4.746116, mean_q: 9.175126, mean_eps: 0.963078\n",
      "   848/20000: episode: 37, duration: 0.095s, episode steps:  20, steps per second: 211, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.067743, mae: 4.583682, mean_q: 8.720623, mean_eps: 0.962312\n",
      "   866/20000: episode: 38, duration: 0.081s, episode steps:  18, steps per second: 222, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.877976, mae: 4.597770, mean_q: 8.774817, mean_eps: 0.961458\n",
      "   910/20000: episode: 39, duration: 0.228s, episode steps:  44, steps per second: 193, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 0.852845, mae: 4.663008, mean_q: 8.915517, mean_eps: 0.960062\n",
      "   925/20000: episode: 40, duration: 0.067s, episode steps:  15, steps per second: 223, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.910580, mae: 5.187079, mean_q: 9.997063, mean_eps: 0.958735\n",
      "   967/20000: episode: 41, duration: 0.202s, episode steps:  42, steps per second: 208, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 0.796817, mae: 5.040394, mean_q: 9.702236, mean_eps: 0.957453\n",
      "   994/20000: episode: 42, duration: 0.140s, episode steps:  27, steps per second: 193, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.764385, mae: 5.042105, mean_q: 9.654595, mean_eps: 0.955900\n",
      "  1006/20000: episode: 43, duration: 0.070s, episode steps:  12, steps per second: 172, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.842082, mae: 5.101551, mean_q: 9.674719, mean_eps: 0.955022\n",
      "  1028/20000: episode: 44, duration: 0.106s, episode steps:  22, steps per second: 207, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.990109, mae: 5.557407, mean_q: 10.599050, mean_eps: 0.954257\n",
      "  1053/20000: episode: 45, duration: 0.122s, episode steps:  25, steps per second: 205, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 0.938330, mae: 5.504030, mean_q: 10.592663, mean_eps: 0.953200\n",
      "  1064/20000: episode: 46, duration: 0.054s, episode steps:  11, steps per second: 204, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.930086, mae: 5.381474, mean_q: 10.262477, mean_eps: 0.952390\n",
      "  1076/20000: episode: 47, duration: 0.058s, episode steps:  12, steps per second: 208, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.861073, mae: 5.407477, mean_q: 10.334492, mean_eps: 0.951873\n",
      "  1104/20000: episode: 48, duration: 0.136s, episode steps:  28, steps per second: 207, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 0.875905, mae: 5.466035, mean_q: 10.481239, mean_eps: 0.950973\n",
      "  1124/20000: episode: 49, duration: 0.096s, episode steps:  20, steps per second: 207, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.647651, mae: 5.891509, mean_q: 11.049149, mean_eps: 0.949893\n",
      "  1137/20000: episode: 50, duration: 0.067s, episode steps:  13, steps per second: 193, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.946157, mae: 5.901486, mean_q: 11.374947, mean_eps: 0.949150\n",
      "  1165/20000: episode: 51, duration: 0.148s, episode steps:  28, steps per second: 189, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.185794, mae: 5.805461, mean_q: 11.088974, mean_eps: 0.948228\n",
      "  1183/20000: episode: 52, duration: 0.103s, episode steps:  18, steps per second: 175, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.895991, mae: 5.858573, mean_q: 11.299155, mean_eps: 0.947192\n",
      "  1198/20000: episode: 53, duration: 0.075s, episode steps:  15, steps per second: 199, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.734541, mae: 5.919031, mean_q: 11.596912, mean_eps: 0.946450\n",
      "  1212/20000: episode: 54, duration: 0.075s, episode steps:  14, steps per second: 187, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.278045, mae: 6.056237, mean_q: 11.614283, mean_eps: 0.945798\n",
      "  1225/20000: episode: 55, duration: 0.065s, episode steps:  13, steps per second: 199, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 1.517485, mae: 6.316808, mean_q: 12.148946, mean_eps: 0.945190\n",
      "  1248/20000: episode: 56, duration: 0.127s, episode steps:  23, steps per second: 182, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.166207, mae: 6.242993, mean_q: 12.117948, mean_eps: 0.944380\n",
      "  1276/20000: episode: 57, duration: 0.149s, episode steps:  28, steps per second: 188, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 0.986889, mae: 6.326293, mean_q: 12.369641, mean_eps: 0.943232\n",
      "  1284/20000: episode: 58, duration: 0.038s, episode steps:   8, steps per second: 208, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 1.138288, mae: 6.173390, mean_q: 12.060772, mean_eps: 0.942422\n",
      "  1299/20000: episode: 59, duration: 0.069s, episode steps:  15, steps per second: 218, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.870279, mae: 6.271129, mean_q: 12.442157, mean_eps: 0.941905\n",
      "  1359/20000: episode: 60, duration: 0.307s, episode steps:  60, steps per second: 196, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 1.347100, mae: 6.615615, mean_q: 12.886029, mean_eps: 0.940217\n",
      "  1381/20000: episode: 61, duration: 0.099s, episode steps:  22, steps per second: 222, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 1.448403, mae: 6.765707, mean_q: 13.192211, mean_eps: 0.938372\n",
      "  1409/20000: episode: 62, duration: 0.139s, episode steps:  28, steps per second: 202, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 1.541472, mae: 6.726317, mean_q: 12.983207, mean_eps: 0.937248\n",
      "  1426/20000: episode: 63, duration: 0.094s, episode steps:  17, steps per second: 181, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 1.289343, mae: 7.011728, mean_q: 13.652462, mean_eps: 0.936235\n",
      "  1451/20000: episode: 64, duration: 0.135s, episode steps:  25, steps per second: 185, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 1.176522, mae: 6.963489, mean_q: 13.661199, mean_eps: 0.935290\n",
      "  1467/20000: episode: 65, duration: 0.079s, episode steps:  16, steps per second: 204, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 1.266132, mae: 6.854795, mean_q: 13.422069, mean_eps: 0.934368\n",
      "  1505/20000: episode: 66, duration: 0.194s, episode steps:  38, steps per second: 196, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.378862, mae: 6.946878, mean_q: 13.664454, mean_eps: 0.933152\n",
      "  1524/20000: episode: 67, duration: 0.104s, episode steps:  19, steps per second: 183, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 1.460170, mae: 7.202651, mean_q: 14.313883, mean_eps: 0.931870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1574/20000: episode: 68, duration: 0.246s, episode steps:  50, steps per second: 203, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.200610, mae: 7.154123, mean_q: 14.194646, mean_eps: 0.930318\n",
      "  1586/20000: episode: 69, duration: 0.060s, episode steps:  12, steps per second: 199, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.211736, mae: 7.140108, mean_q: 14.268094, mean_eps: 0.928923\n",
      "  1604/20000: episode: 70, duration: 0.086s, episode steps:  18, steps per second: 209, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 1.682487, mae: 7.297267, mean_q: 14.405947, mean_eps: 0.928248\n",
      "  1616/20000: episode: 71, duration: 0.059s, episode steps:  12, steps per second: 205, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 1.534702, mae: 7.605048, mean_q: 15.054353, mean_eps: 0.927573\n",
      "  1661/20000: episode: 72, duration: 0.211s, episode steps:  45, steps per second: 213, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.373098, mae: 7.620390, mean_q: 15.171335, mean_eps: 0.926290\n",
      "  1685/20000: episode: 73, duration: 0.119s, episode steps:  24, steps per second: 201, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 1.613233, mae: 7.582154, mean_q: 14.995160, mean_eps: 0.924737\n",
      "  1699/20000: episode: 74, duration: 0.069s, episode steps:  14, steps per second: 202, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.742842, mae: 7.641229, mean_q: 15.225320, mean_eps: 0.923883\n",
      "  1718/20000: episode: 75, duration: 0.100s, episode steps:  19, steps per second: 189, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 1.725291, mae: 7.891550, mean_q: 15.489583, mean_eps: 0.923140\n",
      "  1736/20000: episode: 76, duration: 0.089s, episode steps:  18, steps per second: 202, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.400574, mae: 7.987147, mean_q: 15.999867, mean_eps: 0.922307\n",
      "  1753/20000: episode: 77, duration: 0.084s, episode steps:  17, steps per second: 201, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.852027, mae: 8.028892, mean_q: 16.143612, mean_eps: 0.921520\n",
      "  1778/20000: episode: 78, duration: 0.120s, episode steps:  25, steps per second: 208, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.468156, mae: 7.987101, mean_q: 15.997265, mean_eps: 0.920575\n",
      "  1798/20000: episode: 79, duration: 0.098s, episode steps:  20, steps per second: 204, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.864569, mae: 7.853154, mean_q: 15.597989, mean_eps: 0.919563\n",
      "  1824/20000: episode: 80, duration: 0.132s, episode steps:  26, steps per second: 198, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.833671, mae: 8.344092, mean_q: 16.511918, mean_eps: 0.918527\n",
      "  1839/20000: episode: 81, duration: 0.075s, episode steps:  15, steps per second: 201, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.905783, mae: 8.278691, mean_q: 16.475990, mean_eps: 0.917605\n",
      "  1853/20000: episode: 82, duration: 0.069s, episode steps:  14, steps per second: 203, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.178486, mae: 8.354669, mean_q: 16.677108, mean_eps: 0.916953\n",
      "  1892/20000: episode: 83, duration: 0.192s, episode steps:  39, steps per second: 203, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 1.391002, mae: 8.295048, mean_q: 16.684466, mean_eps: 0.915760\n",
      "  1909/20000: episode: 84, duration: 0.084s, episode steps:  17, steps per second: 203, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.497016, mae: 8.397973, mean_q: 16.726268, mean_eps: 0.914500\n",
      "  1933/20000: episode: 85, duration: 0.119s, episode steps:  24, steps per second: 202, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 1.773385, mae: 8.746330, mean_q: 17.591506, mean_eps: 0.913578\n",
      "  1948/20000: episode: 86, duration: 0.073s, episode steps:  15, steps per second: 205, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.434193, mae: 8.937685, mean_q: 18.100966, mean_eps: 0.912700\n",
      "  1973/20000: episode: 87, duration: 0.119s, episode steps:  25, steps per second: 209, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 1.711859, mae: 8.719550, mean_q: 17.590299, mean_eps: 0.911800\n",
      "  2016/20000: episode: 88, duration: 0.222s, episode steps:  43, steps per second: 193, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 1.618846, mae: 8.843891, mean_q: 17.650795, mean_eps: 0.910270\n",
      "  2031/20000: episode: 89, duration: 0.076s, episode steps:  15, steps per second: 197, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 2.078523, mae: 9.036267, mean_q: 17.999198, mean_eps: 0.908965\n",
      "  2053/20000: episode: 90, duration: 0.107s, episode steps:  22, steps per second: 205, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.591 [0.000, 1.000],  loss: 1.793478, mae: 9.098687, mean_q: 18.249050, mean_eps: 0.908133\n",
      "  2068/20000: episode: 91, duration: 0.077s, episode steps:  15, steps per second: 196, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.999485, mae: 8.900456, mean_q: 17.881071, mean_eps: 0.907300\n",
      "  2080/20000: episode: 92, duration: 0.067s, episode steps:  12, steps per second: 178, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 1.131436, mae: 9.183122, mean_q: 18.609548, mean_eps: 0.906693\n",
      "  2092/20000: episode: 93, duration: 0.063s, episode steps:  12, steps per second: 190, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 1.613987, mae: 9.116104, mean_q: 18.369972, mean_eps: 0.906153\n",
      "  2111/20000: episode: 94, duration: 0.096s, episode steps:  19, steps per second: 198, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.684 [0.000, 1.000],  loss: 2.010930, mae: 9.023905, mean_q: 18.001729, mean_eps: 0.905455\n",
      "  2124/20000: episode: 95, duration: 0.063s, episode steps:  13, steps per second: 206, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 1.952757, mae: 9.275414, mean_q: 18.708771, mean_eps: 0.904735\n",
      "  2145/20000: episode: 96, duration: 0.102s, episode steps:  21, steps per second: 207, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 1.748065, mae: 9.327186, mean_q: 18.826982, mean_eps: 0.903970\n",
      "  2176/20000: episode: 97, duration: 0.142s, episode steps:  31, steps per second: 218, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 1.632729, mae: 9.338725, mean_q: 18.834098, mean_eps: 0.902800\n",
      "  2196/20000: episode: 98, duration: 0.092s, episode steps:  20, steps per second: 217, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 1.719483, mae: 9.307521, mean_q: 18.710991, mean_eps: 0.901652\n",
      "  2213/20000: episode: 99, duration: 0.082s, episode steps:  17, steps per second: 208, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 2.481330, mae: 9.499441, mean_q: 18.874146, mean_eps: 0.900820\n",
      "  2282/20000: episode: 100, duration: 0.332s, episode steps:  69, steps per second: 208, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 1.630746, mae: 9.754544, mean_q: 19.692038, mean_eps: 0.898885\n",
      "  2301/20000: episode: 101, duration: 0.096s, episode steps:  19, steps per second: 197, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 2.444471, mae: 9.701953, mean_q: 19.520590, mean_eps: 0.896905\n",
      "  2322/20000: episode: 102, duration: 0.110s, episode steps:  21, steps per second: 192, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 2.571707, mae: 10.104780, mean_q: 20.239916, mean_eps: 0.896005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2356/20000: episode: 103, duration: 0.159s, episode steps:  34, steps per second: 214, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 1.515708, mae: 10.072885, mean_q: 20.355918, mean_eps: 0.894767\n",
      "  2380/20000: episode: 104, duration: 0.117s, episode steps:  24, steps per second: 204, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 1.731526, mae: 10.080506, mean_q: 20.476546, mean_eps: 0.893462\n",
      "  2410/20000: episode: 105, duration: 0.144s, episode steps:  30, steps per second: 209, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.709024, mae: 10.067819, mean_q: 20.501375, mean_eps: 0.892247\n",
      "  2441/20000: episode: 106, duration: 0.145s, episode steps:  31, steps per second: 213, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  loss: 1.853502, mae: 10.726273, mean_q: 21.785504, mean_eps: 0.890875\n",
      "  2459/20000: episode: 107, duration: 0.092s, episode steps:  18, steps per second: 197, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.328962, mae: 10.658869, mean_q: 21.705625, mean_eps: 0.889772\n",
      "  2504/20000: episode: 108, duration: 0.205s, episode steps:  45, steps per second: 219, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 2.359222, mae: 10.542045, mean_q: 21.305675, mean_eps: 0.888355\n",
      "  2526/20000: episode: 109, duration: 0.115s, episode steps:  22, steps per second: 192, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 2.008723, mae: 10.850884, mean_q: 21.915955, mean_eps: 0.886848\n",
      "  2542/20000: episode: 110, duration: 0.076s, episode steps:  16, steps per second: 211, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.285124, mae: 11.266686, mean_q: 22.823508, mean_eps: 0.885992\n",
      "  2562/20000: episode: 111, duration: 0.109s, episode steps:  20, steps per second: 183, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 2.434794, mae: 10.989360, mean_q: 22.192748, mean_eps: 0.885182\n",
      "  2593/20000: episode: 112, duration: 0.170s, episode steps:  31, steps per second: 183, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 1.867535, mae: 10.954329, mean_q: 22.389531, mean_eps: 0.884035\n",
      "  2609/20000: episode: 113, duration: 0.089s, episode steps:  16, steps per second: 180, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 2.013847, mae: 11.003845, mean_q: 22.361900, mean_eps: 0.882977\n",
      "  2636/20000: episode: 114, duration: 0.125s, episode steps:  27, steps per second: 215, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  loss: 1.472178, mae: 11.250667, mean_q: 23.019789, mean_eps: 0.882010\n",
      "  2666/20000: episode: 115, duration: 0.140s, episode steps:  30, steps per second: 214, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.922164, mae: 11.283117, mean_q: 23.011830, mean_eps: 0.880728\n",
      "  2683/20000: episode: 116, duration: 0.084s, episode steps:  17, steps per second: 202, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 1.460577, mae: 11.319527, mean_q: 23.032687, mean_eps: 0.879670\n",
      "  2747/20000: episode: 117, duration: 0.300s, episode steps:  64, steps per second: 214, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 2.148913, mae: 11.676023, mean_q: 23.723071, mean_eps: 0.877848\n",
      "  2800/20000: episode: 118, duration: 0.256s, episode steps:  53, steps per second: 207, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 1.722049, mae: 11.844627, mean_q: 24.263712, mean_eps: 0.875215\n",
      "  2846/20000: episode: 119, duration: 0.225s, episode steps:  46, steps per second: 204, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.351411, mae: 12.154996, mean_q: 25.013083, mean_eps: 0.872987\n",
      "  2899/20000: episode: 120, duration: 0.252s, episode steps:  53, steps per second: 210, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 2.289710, mae: 12.214812, mean_q: 25.057964, mean_eps: 0.870760\n",
      "  2949/20000: episode: 121, duration: 0.235s, episode steps:  50, steps per second: 213, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 2.519773, mae: 13.085531, mean_q: 26.638630, mean_eps: 0.868443\n",
      "  2979/20000: episode: 122, duration: 0.146s, episode steps:  30, steps per second: 206, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.359459, mae: 13.100616, mean_q: 26.711725, mean_eps: 0.866642\n",
      "  2996/20000: episode: 123, duration: 0.084s, episode steps:  17, steps per second: 202, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 1.978288, mae: 13.260596, mean_q: 26.999902, mean_eps: 0.865585\n",
      "  3013/20000: episode: 124, duration: 0.090s, episode steps:  17, steps per second: 189, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 1.810535, mae: 13.408985, mean_q: 27.458841, mean_eps: 0.864820\n",
      "  3031/20000: episode: 125, duration: 0.097s, episode steps:  18, steps per second: 186, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 1.417588, mae: 13.439782, mean_q: 27.581987, mean_eps: 0.864032\n",
      "  3062/20000: episode: 126, duration: 0.185s, episode steps:  31, steps per second: 168, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  loss: 2.073663, mae: 13.290885, mean_q: 27.283503, mean_eps: 0.862930\n",
      "  3149/20000: episode: 127, duration: 0.447s, episode steps:  87, steps per second: 195, episode reward: 87.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 2.392450, mae: 13.501084, mean_q: 27.549218, mean_eps: 0.860275\n",
      "  3177/20000: episode: 128, duration: 0.129s, episode steps:  28, steps per second: 217, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 1.675725, mae: 13.431547, mean_q: 27.588931, mean_eps: 0.857687\n",
      "  3187/20000: episode: 129, duration: 0.057s, episode steps:  10, steps per second: 175, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 2.455225, mae: 13.867379, mean_q: 28.164556, mean_eps: 0.856832\n",
      "  3205/20000: episode: 130, duration: 0.102s, episode steps:  18, steps per second: 177, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.278 [0.000, 1.000],  loss: 1.983411, mae: 13.728142, mean_q: 28.044787, mean_eps: 0.856202\n",
      "  3217/20000: episode: 131, duration: 0.063s, episode steps:  12, steps per second: 192, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.668181, mae: 14.710402, mean_q: 29.896549, mean_eps: 0.855527\n",
      "  3242/20000: episode: 132, duration: 0.152s, episode steps:  25, steps per second: 165, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.320 [0.000, 1.000],  loss: 2.990921, mae: 14.318871, mean_q: 29.126781, mean_eps: 0.854695\n",
      "  3321/20000: episode: 133, duration: 0.391s, episode steps:  79, steps per second: 202, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 2.043837, mae: 14.306749, mean_q: 29.218835, mean_eps: 0.852355\n",
      "  3333/20000: episode: 134, duration: 0.063s, episode steps:  12, steps per second: 190, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 2.513889, mae: 14.540606, mean_q: 29.817283, mean_eps: 0.850307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3389/20000: episode: 135, duration: 0.300s, episode steps:  56, steps per second: 187, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.554 [0.000, 1.000],  loss: 2.610476, mae: 14.647402, mean_q: 29.914601, mean_eps: 0.848777\n",
      "  3435/20000: episode: 136, duration: 0.395s, episode steps:  46, steps per second: 116, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 2.681443, mae: 15.172610, mean_q: 30.867996, mean_eps: 0.846482\n",
      "  3468/20000: episode: 137, duration: 0.209s, episode steps:  33, steps per second: 158, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 3.072232, mae: 15.345224, mean_q: 31.222156, mean_eps: 0.844705\n",
      "  3505/20000: episode: 138, duration: 0.234s, episode steps:  37, steps per second: 158, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  loss: 1.925163, mae: 15.527442, mean_q: 31.670290, mean_eps: 0.843130\n",
      "  3556/20000: episode: 139, duration: 0.290s, episode steps:  51, steps per second: 176, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 2.456765, mae: 15.737720, mean_q: 32.132322, mean_eps: 0.841150\n",
      "  3578/20000: episode: 140, duration: 0.145s, episode steps:  22, steps per second: 152, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.558995, mae: 15.592684, mean_q: 31.839915, mean_eps: 0.839507\n",
      "  3626/20000: episode: 141, duration: 0.324s, episode steps:  48, steps per second: 148, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 2.707828, mae: 16.052339, mean_q: 32.884180, mean_eps: 0.837932\n",
      "  3636/20000: episode: 142, duration: 0.074s, episode steps:  10, steps per second: 136, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 3.386892, mae: 16.081873, mean_q: 32.702981, mean_eps: 0.836627\n",
      "  3673/20000: episode: 143, duration: 0.215s, episode steps:  37, steps per second: 172, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.941248, mae: 16.112525, mean_q: 33.221328, mean_eps: 0.835570\n",
      "  3686/20000: episode: 144, duration: 0.071s, episode steps:  13, steps per second: 183, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 1.963151, mae: 16.214394, mean_q: 33.440422, mean_eps: 0.834445\n",
      "  3708/20000: episode: 145, duration: 0.128s, episode steps:  22, steps per second: 172, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 2.996962, mae: 16.142807, mean_q: 33.191903, mean_eps: 0.833658\n",
      "  3724/20000: episode: 146, duration: 0.196s, episode steps:  16, steps per second:  82, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 3.103675, mae: 16.335860, mean_q: 33.596274, mean_eps: 0.832803\n",
      "  3757/20000: episode: 147, duration: 0.316s, episode steps:  33, steps per second: 104, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 3.701613, mae: 16.443780, mean_q: 33.571783, mean_eps: 0.831700\n",
      "  3787/20000: episode: 148, duration: 0.292s, episode steps:  30, steps per second: 103, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.567 [0.000, 1.000],  loss: 2.640642, mae: 16.465452, mean_q: 33.774059, mean_eps: 0.830282\n",
      "  3885/20000: episode: 149, duration: 0.611s, episode steps:  98, steps per second: 160, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.266495, mae: 16.927967, mean_q: 34.564199, mean_eps: 0.827403\n",
      "  3937/20000: episode: 150, duration: 0.285s, episode steps:  52, steps per second: 182, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 3.135388, mae: 17.268003, mean_q: 35.257492, mean_eps: 0.824027\n",
      "  3969/20000: episode: 151, duration: 0.169s, episode steps:  32, steps per second: 189, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 2.273589, mae: 17.644053, mean_q: 36.220528, mean_eps: 0.822137\n",
      "  4022/20000: episode: 152, duration: 0.289s, episode steps:  53, steps per second: 184, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 3.302163, mae: 17.467624, mean_q: 35.980434, mean_eps: 0.820225\n",
      "  4037/20000: episode: 153, duration: 0.082s, episode steps:  15, steps per second: 183, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 2.726950, mae: 18.196024, mean_q: 37.287126, mean_eps: 0.818695\n",
      "  4053/20000: episode: 154, duration: 0.091s, episode steps:  16, steps per second: 176, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 2.235342, mae: 17.525631, mean_q: 36.221000, mean_eps: 0.817997\n",
      "  4085/20000: episode: 155, duration: 0.181s, episode steps:  32, steps per second: 177, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.437253, mae: 17.988884, mean_q: 37.382796, mean_eps: 0.816917\n",
      "  4161/20000: episode: 156, duration: 0.422s, episode steps:  76, steps per second: 180, episode reward: 76.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 3.780010, mae: 18.208989, mean_q: 37.528628, mean_eps: 0.814488\n",
      "  4179/20000: episode: 157, duration: 0.104s, episode steps:  18, steps per second: 173, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 3.502613, mae: 18.593410, mean_q: 38.187317, mean_eps: 0.812372\n",
      "  4194/20000: episode: 158, duration: 0.085s, episode steps:  15, steps per second: 175, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.588167, mae: 18.481629, mean_q: 37.733110, mean_eps: 0.811630\n",
      "  4244/20000: episode: 159, duration: 0.281s, episode steps:  50, steps per second: 178, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.198091, mae: 18.713732, mean_q: 38.527924, mean_eps: 0.810167\n",
      "  4286/20000: episode: 160, duration: 0.247s, episode steps:  42, steps per second: 170, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.110388, mae: 18.520967, mean_q: 38.184524, mean_eps: 0.808098\n",
      "  4327/20000: episode: 161, duration: 0.227s, episode steps:  41, steps per second: 180, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.561 [0.000, 1.000],  loss: 2.982465, mae: 19.352289, mean_q: 39.839703, mean_eps: 0.806230\n",
      "  4343/20000: episode: 162, duration: 0.094s, episode steps:  16, steps per second: 171, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 3.170385, mae: 19.226294, mean_q: 39.480777, mean_eps: 0.804947\n",
      "  4398/20000: episode: 163, duration: 0.295s, episode steps:  55, steps per second: 186, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.436 [0.000, 1.000],  loss: 2.503066, mae: 19.136579, mean_q: 39.567346, mean_eps: 0.803350\n",
      "  4421/20000: episode: 164, duration: 0.134s, episode steps:  23, steps per second: 171, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 4.621823, mae: 19.705929, mean_q: 40.351343, mean_eps: 0.801595\n",
      "  4479/20000: episode: 165, duration: 0.330s, episode steps:  58, steps per second: 176, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 3.099240, mae: 19.936583, mean_q: 41.095933, mean_eps: 0.799772\n",
      "  4489/20000: episode: 166, duration: 0.059s, episode steps:  10, steps per second: 170, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 3.704230, mae: 20.171102, mean_q: 41.424001, mean_eps: 0.798243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4532/20000: episode: 167, duration: 0.240s, episode steps:  43, steps per second: 179, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  loss: 3.473812, mae: 20.297570, mean_q: 41.695896, mean_eps: 0.797050\n",
      "  4553/20000: episode: 168, duration: 0.117s, episode steps:  21, steps per second: 180, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 3.351155, mae: 20.684738, mean_q: 42.539342, mean_eps: 0.795610\n",
      "  4601/20000: episode: 169, duration: 0.271s, episode steps:  48, steps per second: 177, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.458 [0.000, 1.000],  loss: 3.847457, mae: 20.578981, mean_q: 42.386427, mean_eps: 0.794057\n",
      "  4668/20000: episode: 170, duration: 0.376s, episode steps:  67, steps per second: 178, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 4.711332, mae: 21.070056, mean_q: 43.280575, mean_eps: 0.791470\n",
      "  4728/20000: episode: 171, duration: 0.346s, episode steps:  60, steps per second: 173, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 4.085039, mae: 21.231417, mean_q: 43.847890, mean_eps: 0.788612\n",
      "  4759/20000: episode: 172, duration: 0.166s, episode steps:  31, steps per second: 186, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.581 [0.000, 1.000],  loss: 4.084036, mae: 21.395141, mean_q: 44.109019, mean_eps: 0.786565\n",
      "  4779/20000: episode: 173, duration: 0.112s, episode steps:  20, steps per second: 179, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.626047, mae: 21.404672, mean_q: 43.978570, mean_eps: 0.785418\n",
      "  4887/20000: episode: 174, duration: 0.594s, episode steps: 108, steps per second: 182, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 4.836302, mae: 21.812285, mean_q: 44.951011, mean_eps: 0.782537\n",
      "  4909/20000: episode: 175, duration: 0.126s, episode steps:  22, steps per second: 175, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 4.295211, mae: 22.011038, mean_q: 45.319740, mean_eps: 0.779612\n",
      "  4987/20000: episode: 176, duration: 0.437s, episode steps:  78, steps per second: 179, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.590 [0.000, 1.000],  loss: 4.526481, mae: 22.377536, mean_q: 46.130550, mean_eps: 0.777363\n",
      "  5036/20000: episode: 177, duration: 0.290s, episode steps:  49, steps per second: 169, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.070067, mae: 22.573858, mean_q: 46.404287, mean_eps: 0.774505\n",
      "  5063/20000: episode: 178, duration: 0.158s, episode steps:  27, steps per second: 170, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.593 [0.000, 1.000],  loss: 5.336127, mae: 22.998637, mean_q: 47.235657, mean_eps: 0.772795\n",
      "  5073/20000: episode: 179, duration: 0.065s, episode steps:  10, steps per second: 155, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 6.117201, mae: 23.000664, mean_q: 47.533266, mean_eps: 0.771962\n",
      "  5110/20000: episode: 180, duration: 0.207s, episode steps:  37, steps per second: 179, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.432 [0.000, 1.000],  loss: 4.750069, mae: 22.841962, mean_q: 47.318251, mean_eps: 0.770905\n",
      "  5133/20000: episode: 181, duration: 0.133s, episode steps:  23, steps per second: 173, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 5.370242, mae: 23.733632, mean_q: 49.269651, mean_eps: 0.769555\n",
      "  5208/20000: episode: 182, duration: 0.416s, episode steps:  75, steps per second: 180, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 4.872066, mae: 23.542865, mean_q: 48.586979, mean_eps: 0.767350\n",
      "  5248/20000: episode: 183, duration: 0.219s, episode steps:  40, steps per second: 182, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.425 [0.000, 1.000],  loss: 7.999552, mae: 23.883007, mean_q: 49.346048, mean_eps: 0.764763\n",
      "  5262/20000: episode: 184, duration: 0.080s, episode steps:  14, steps per second: 175, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 2.606958, mae: 23.573185, mean_q: 48.876408, mean_eps: 0.763547\n",
      "  5287/20000: episode: 185, duration: 0.143s, episode steps:  25, steps per second: 175, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 4.125175, mae: 23.999943, mean_q: 49.732283, mean_eps: 0.762670\n",
      "  5307/20000: episode: 186, duration: 0.118s, episode steps:  20, steps per second: 169, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 6.289014, mae: 24.169931, mean_q: 49.755919, mean_eps: 0.761657\n",
      "  5351/20000: episode: 187, duration: 0.254s, episode steps:  44, steps per second: 173, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 6.188642, mae: 24.524675, mean_q: 50.419388, mean_eps: 0.760218\n",
      "  5362/20000: episode: 188, duration: 0.066s, episode steps:  11, steps per second: 168, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 4.303532, mae: 24.786917, mean_q: 50.921077, mean_eps: 0.758980\n",
      "  5442/20000: episode: 189, duration: 0.445s, episode steps:  80, steps per second: 180, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 4.387475, mae: 24.638914, mean_q: 50.832470, mean_eps: 0.756932\n",
      "  5464/20000: episode: 190, duration: 0.125s, episode steps:  22, steps per second: 176, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 3.776217, mae: 25.264721, mean_q: 52.157974, mean_eps: 0.754638\n",
      "  5506/20000: episode: 191, duration: 0.242s, episode steps:  42, steps per second: 174, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.452 [0.000, 1.000],  loss: 7.334991, mae: 25.144649, mean_q: 51.784980, mean_eps: 0.753198\n",
      "  5526/20000: episode: 192, duration: 0.112s, episode steps:  20, steps per second: 179, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 6.003127, mae: 25.688685, mean_q: 52.962402, mean_eps: 0.751802\n",
      "  5559/20000: episode: 193, duration: 0.191s, episode steps:  33, steps per second: 173, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.576 [0.000, 1.000],  loss: 4.819483, mae: 25.906122, mean_q: 53.235455, mean_eps: 0.750610\n",
      "  5651/20000: episode: 194, duration: 0.504s, episode steps:  92, steps per second: 183, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 6.451135, mae: 26.395872, mean_q: 54.185529, mean_eps: 0.747798\n",
      "  5726/20000: episode: 195, duration: 0.416s, episode steps:  75, steps per second: 180, episode reward: 75.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 4.572457, mae: 26.487827, mean_q: 54.769840, mean_eps: 0.744040\n",
      "  5767/20000: episode: 196, duration: 0.230s, episode steps:  41, steps per second: 178, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 6.708089, mae: 26.601242, mean_q: 55.055176, mean_eps: 0.741430\n",
      "  5806/20000: episode: 197, duration: 0.219s, episode steps:  39, steps per second: 178, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 4.892145, mae: 26.733171, mean_q: 55.180259, mean_eps: 0.739630\n",
      "  5816/20000: episode: 198, duration: 0.063s, episode steps:  10, steps per second: 158, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 11.320300, mae: 27.382736, mean_q: 56.056500, mean_eps: 0.738528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5851/20000: episode: 199, duration: 0.199s, episode steps:  35, steps per second: 176, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 6.218999, mae: 27.383760, mean_q: 56.259870, mean_eps: 0.737515\n",
      "  5920/20000: episode: 200, duration: 0.386s, episode steps:  69, steps per second: 179, episode reward: 69.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  loss: 4.715307, mae: 27.955151, mean_q: 57.984375, mean_eps: 0.735175\n",
      "  5959/20000: episode: 201, duration: 0.219s, episode steps:  39, steps per second: 178, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.436 [0.000, 1.000],  loss: 7.794355, mae: 28.358964, mean_q: 58.653961, mean_eps: 0.732745\n",
      "  6004/20000: episode: 202, duration: 0.250s, episode steps:  45, steps per second: 180, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 6.781299, mae: 28.821886, mean_q: 59.465913, mean_eps: 0.730855\n",
      "  6044/20000: episode: 203, duration: 0.226s, episode steps:  40, steps per second: 177, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.823232, mae: 29.442825, mean_q: 60.579977, mean_eps: 0.728942\n",
      "  6059/20000: episode: 204, duration: 0.097s, episode steps:  15, steps per second: 155, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 4.321346, mae: 29.598014, mean_q: 61.286640, mean_eps: 0.727705\n",
      "  6084/20000: episode: 205, duration: 0.149s, episode steps:  25, steps per second: 168, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 4.233374, mae: 29.797801, mean_q: 61.594095, mean_eps: 0.726805\n",
      "  6183/20000: episode: 206, duration: 0.543s, episode steps:  99, steps per second: 182, episode reward: 99.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 8.899569, mae: 29.817592, mean_q: 61.493246, mean_eps: 0.724015\n",
      "  6260/20000: episode: 207, duration: 0.424s, episode steps:  77, steps per second: 182, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 9.109151, mae: 30.117424, mean_q: 62.186276, mean_eps: 0.720055\n",
      "  6308/20000: episode: 208, duration: 0.276s, episode steps:  48, steps per second: 174, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 6.527381, mae: 30.807482, mean_q: 63.414207, mean_eps: 0.717243\n",
      "  6344/20000: episode: 209, duration: 0.201s, episode steps:  36, steps per second: 179, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 6.984468, mae: 31.250804, mean_q: 64.470403, mean_eps: 0.715353\n",
      "  6382/20000: episode: 210, duration: 0.218s, episode steps:  38, steps per second: 174, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 10.195555, mae: 31.528138, mean_q: 64.533778, mean_eps: 0.713688\n",
      "  6422/20000: episode: 211, duration: 0.223s, episode steps:  40, steps per second: 180, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 9.493806, mae: 31.718653, mean_q: 65.057188, mean_eps: 0.711932\n",
      "  6435/20000: episode: 212, duration: 0.076s, episode steps:  13, steps per second: 170, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 9.694780, mae: 31.138280, mean_q: 63.591912, mean_eps: 0.710740\n",
      "  6462/20000: episode: 213, duration: 0.156s, episode steps:  27, steps per second: 174, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.407 [0.000, 1.000],  loss: 8.873356, mae: 31.681697, mean_q: 65.057118, mean_eps: 0.709840\n",
      "  6481/20000: episode: 214, duration: 0.109s, episode steps:  19, steps per second: 174, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 9.244213, mae: 31.526290, mean_q: 64.426604, mean_eps: 0.708805\n",
      "  6501/20000: episode: 215, duration: 0.115s, episode steps:  20, steps per second: 174, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 13.168462, mae: 31.651830, mean_q: 64.777739, mean_eps: 0.707927\n",
      "  6557/20000: episode: 216, duration: 0.307s, episode steps:  56, steps per second: 182, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 10.444833, mae: 32.344430, mean_q: 66.142327, mean_eps: 0.706218\n",
      "  6661/20000: episode: 217, duration: 0.584s, episode steps: 104, steps per second: 178, episode reward: 104.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 8.825026, mae: 31.845986, mean_q: 65.474368, mean_eps: 0.702618\n",
      "  6692/20000: episode: 218, duration: 0.171s, episode steps:  31, steps per second: 181, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 9.291099, mae: 32.146773, mean_q: 66.336542, mean_eps: 0.699580\n",
      "  6775/20000: episode: 219, duration: 0.456s, episode steps:  83, steps per second: 182, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 9.577923, mae: 32.401633, mean_q: 66.611770, mean_eps: 0.697015\n",
      "  6794/20000: episode: 220, duration: 0.113s, episode steps:  19, steps per second: 169, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.684 [0.000, 1.000],  loss: 12.306427, mae: 32.540740, mean_q: 67.045172, mean_eps: 0.694720\n",
      "  6871/20000: episode: 221, duration: 0.427s, episode steps:  77, steps per second: 180, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 12.123036, mae: 33.106084, mean_q: 68.243698, mean_eps: 0.692560\n",
      "  6986/20000: episode: 222, duration: 0.641s, episode steps: 115, steps per second: 179, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 7.982951, mae: 33.559610, mean_q: 69.241241, mean_eps: 0.688240\n",
      "  7030/20000: episode: 223, duration: 0.244s, episode steps:  44, steps per second: 180, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 11.502352, mae: 34.278439, mean_q: 70.538556, mean_eps: 0.684662\n",
      "  7177/20000: episode: 224, duration: 0.808s, episode steps: 147, steps per second: 182, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 8.145017, mae: 35.223426, mean_q: 72.829353, mean_eps: 0.680365\n",
      "  7206/20000: episode: 225, duration: 0.163s, episode steps:  29, steps per second: 178, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.414 [0.000, 1.000],  loss: 10.367591, mae: 35.456677, mean_q: 73.149691, mean_eps: 0.676405\n",
      "  7223/20000: episode: 226, duration: 0.102s, episode steps:  17, steps per second: 167, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 7.767850, mae: 36.120502, mean_q: 74.472396, mean_eps: 0.675370\n",
      "  7272/20000: episode: 227, duration: 0.264s, episode steps:  49, steps per second: 185, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 9.581010, mae: 36.196222, mean_q: 74.889746, mean_eps: 0.673885\n",
      "  7282/20000: episode: 228, duration: 0.058s, episode steps:  10, steps per second: 172, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 12.816971, mae: 35.754092, mean_q: 74.469714, mean_eps: 0.672557\n",
      "  7322/20000: episode: 229, duration: 0.228s, episode steps:  40, steps per second: 175, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.575 [0.000, 1.000],  loss: 11.083440, mae: 36.294931, mean_q: 74.520246, mean_eps: 0.671433\n",
      "  7367/20000: episode: 230, duration: 0.256s, episode steps:  45, steps per second: 176, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 11.264257, mae: 36.723701, mean_q: 75.497029, mean_eps: 0.669520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7438/20000: episode: 231, duration: 0.392s, episode steps:  71, steps per second: 181, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 13.017635, mae: 37.047396, mean_q: 76.195785, mean_eps: 0.666910\n",
      "  7499/20000: episode: 232, duration: 0.344s, episode steps:  61, steps per second: 177, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 9.769186, mae: 37.472089, mean_q: 77.078676, mean_eps: 0.663940\n",
      "  7515/20000: episode: 233, duration: 0.092s, episode steps:  16, steps per second: 174, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 8.064774, mae: 37.786335, mean_q: 77.631427, mean_eps: 0.662208\n",
      "  7529/20000: episode: 234, duration: 0.085s, episode steps:  14, steps per second: 165, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 12.874736, mae: 38.299038, mean_q: 78.474729, mean_eps: 0.661532\n",
      "  7563/20000: episode: 235, duration: 0.195s, episode steps:  34, steps per second: 175, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 11.684773, mae: 37.942099, mean_q: 77.924951, mean_eps: 0.660452\n",
      "  7659/20000: episode: 236, duration: 0.536s, episode steps:  96, steps per second: 179, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 13.357261, mae: 37.836111, mean_q: 77.672715, mean_eps: 0.657528\n",
      "  7705/20000: episode: 237, duration: 0.253s, episode steps:  46, steps per second: 182, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 10.335403, mae: 38.512688, mean_q: 79.202245, mean_eps: 0.654333\n",
      "  7759/20000: episode: 238, duration: 0.297s, episode steps:  54, steps per second: 182, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 12.988513, mae: 39.614845, mean_q: 81.031243, mean_eps: 0.652082\n",
      "  7812/20000: episode: 239, duration: 0.296s, episode steps:  53, steps per second: 179, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.434 [0.000, 1.000],  loss: 14.731529, mae: 39.103828, mean_q: 80.148618, mean_eps: 0.649675\n",
      "  7959/20000: episode: 240, duration: 0.810s, episode steps: 147, steps per second: 181, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 10.723169, mae: 40.033112, mean_q: 82.213856, mean_eps: 0.645175\n",
      "  8085/20000: episode: 241, duration: 0.698s, episode steps: 126, steps per second: 180, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 13.132514, mae: 40.749824, mean_q: 83.681085, mean_eps: 0.639032\n",
      "  8245/20000: episode: 242, duration: 0.904s, episode steps: 160, steps per second: 177, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 16.269195, mae: 41.639038, mean_q: 85.373205, mean_eps: 0.632597\n",
      "  8404/20000: episode: 243, duration: 0.971s, episode steps: 159, steps per second: 164, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 16.773632, mae: 42.606256, mean_q: 87.357299, mean_eps: 0.625420\n",
      "  8589/20000: episode: 244, duration: 1.069s, episode steps: 185, steps per second: 173, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 15.031228, mae: 44.347477, mean_q: 90.921828, mean_eps: 0.617680\n",
      "  8694/20000: episode: 245, duration: 0.600s, episode steps: 105, steps per second: 175, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 13.597239, mae: 45.117843, mean_q: 92.546399, mean_eps: 0.611155\n",
      "  8761/20000: episode: 246, duration: 0.359s, episode steps:  67, steps per second: 187, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 17.069120, mae: 45.771917, mean_q: 93.998705, mean_eps: 0.607285\n",
      "  8912/20000: episode: 247, duration: 0.826s, episode steps: 151, steps per second: 183, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 17.021247, mae: 46.810086, mean_q: 96.026169, mean_eps: 0.602380\n",
      "  9023/20000: episode: 248, duration: 0.608s, episode steps: 111, steps per second: 183, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 20.140892, mae: 48.135133, mean_q: 98.735427, mean_eps: 0.596485\n",
      "  9291/20000: episode: 249, duration: 1.451s, episode steps: 268, steps per second: 185, episode reward: 268.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 20.929728, mae: 49.080013, mean_q: 100.667167, mean_eps: 0.587958\n",
      "  9571/20000: episode: 250, duration: 1.531s, episode steps: 280, steps per second: 183, episode reward: 280.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 21.557062, mae: 50.346551, mean_q: 103.421234, mean_eps: 0.575628\n",
      "  9625/20000: episode: 251, duration: 0.304s, episode steps:  54, steps per second: 177, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 17.295839, mae: 50.937340, mean_q: 104.776484, mean_eps: 0.568113\n",
      "  9644/20000: episode: 252, duration: 0.111s, episode steps:  19, steps per second: 171, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 11.199557, mae: 51.181061, mean_q: 105.376163, mean_eps: 0.566470\n",
      "  9703/20000: episode: 253, duration: 0.325s, episode steps:  59, steps per second: 182, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 21.691668, mae: 51.944295, mean_q: 106.740458, mean_eps: 0.564715\n",
      "  9839/20000: episode: 254, duration: 0.742s, episode steps: 136, steps per second: 183, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 16.550193, mae: 52.587658, mean_q: 107.787209, mean_eps: 0.560328\n",
      "  9961/20000: episode: 255, duration: 0.672s, episode steps: 122, steps per second: 182, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 19.412616, mae: 53.160501, mean_q: 109.022502, mean_eps: 0.554523\n",
      " 10032/20000: episode: 256, duration: 0.404s, episode steps:  71, steps per second: 176, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.563 [0.000, 1.000],  loss: 21.722688, mae: 53.795873, mean_q: 110.204250, mean_eps: 0.550180\n",
      " 10100/20000: episode: 257, duration: 0.377s, episode steps:  68, steps per second: 180, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 15.716840, mae: 54.010154, mean_q: 110.725240, mean_eps: 0.547052\n",
      " 10217/20000: episode: 258, duration: 0.634s, episode steps: 117, steps per second: 185, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 18.875781, mae: 54.950247, mean_q: 112.672603, mean_eps: 0.542890\n",
      " 10276/20000: episode: 259, duration: 0.338s, episode steps:  59, steps per second: 175, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 12.502149, mae: 55.981526, mean_q: 114.934304, mean_eps: 0.538930\n",
      " 10335/20000: episode: 260, duration: 0.324s, episode steps:  59, steps per second: 182, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 19.332122, mae: 55.903233, mean_q: 114.331305, mean_eps: 0.536275\n",
      " 10442/20000: episode: 261, duration: 0.597s, episode steps: 107, steps per second: 179, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 26.922172, mae: 56.297199, mean_q: 115.126282, mean_eps: 0.532540\n",
      " 10461/20000: episode: 262, duration: 0.111s, episode steps:  19, steps per second: 171, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 18.392263, mae: 55.876744, mean_q: 115.027369, mean_eps: 0.529705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10587/20000: episode: 263, duration: 0.693s, episode steps: 126, steps per second: 182, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 17.021018, mae: 56.708253, mean_q: 116.351348, mean_eps: 0.526442\n",
      " 10671/20000: episode: 264, duration: 0.464s, episode steps:  84, steps per second: 181, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 18.562899, mae: 57.647798, mean_q: 118.195018, mean_eps: 0.521717\n",
      " 10968/20000: episode: 265, duration: 1.607s, episode steps: 297, steps per second: 185, episode reward: 297.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 24.310780, mae: 58.692091, mean_q: 119.797981, mean_eps: 0.513145\n",
      " 11249/20000: episode: 266, duration: 1.544s, episode steps: 281, steps per second: 182, episode reward: 281.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 22.790349, mae: 60.529353, mean_q: 123.874483, mean_eps: 0.500140\n",
      " 11302/20000: episode: 267, duration: 0.296s, episode steps:  53, steps per second: 179, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 20.378941, mae: 61.674336, mean_q: 126.716749, mean_eps: 0.492625\n",
      " 11623/20000: episode: 268, duration: 1.761s, episode steps: 321, steps per second: 182, episode reward: 321.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 21.828048, mae: 63.143525, mean_q: 129.207506, mean_eps: 0.484210\n",
      " 11765/20000: episode: 269, duration: 0.773s, episode steps: 142, steps per second: 184, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 33.185446, mae: 64.764218, mean_q: 131.981130, mean_eps: 0.473792\n",
      " 11967/20000: episode: 270, duration: 1.109s, episode steps: 202, steps per second: 182, episode reward: 202.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 28.229897, mae: 65.949955, mean_q: 134.775788, mean_eps: 0.466053\n",
      " 12013/20000: episode: 271, duration: 0.256s, episode steps:  46, steps per second: 180, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 24.454633, mae: 66.944608, mean_q: 136.044219, mean_eps: 0.460473\n",
      " 12029/20000: episode: 272, duration: 0.093s, episode steps:  16, steps per second: 173, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 42.679337, mae: 66.618437, mean_q: 135.735269, mean_eps: 0.459077\n",
      " 12353/20000: episode: 273, duration: 1.799s, episode steps: 324, steps per second: 180, episode reward: 324.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 31.328069, mae: 67.918403, mean_q: 138.798950, mean_eps: 0.451427\n",
      " 12632/20000: episode: 274, duration: 1.550s, episode steps: 279, steps per second: 180, episode reward: 279.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 37.685572, mae: 69.315678, mean_q: 141.526368, mean_eps: 0.437860\n",
      " 12943/20000: episode: 275, duration: 1.720s, episode steps: 311, steps per second: 181, episode reward: 311.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 32.318423, mae: 70.224949, mean_q: 143.560733, mean_eps: 0.424585\n",
      " 13058/20000: episode: 276, duration: 0.641s, episode steps: 115, steps per second: 179, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 25.794974, mae: 71.513290, mean_q: 146.144110, mean_eps: 0.415000\n",
      " 13108/20000: episode: 277, duration: 0.290s, episode steps:  50, steps per second: 172, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.580 [0.000, 1.000],  loss: 32.307674, mae: 72.579907, mean_q: 148.581747, mean_eps: 0.411287\n",
      " 13162/20000: episode: 278, duration: 0.294s, episode steps:  54, steps per second: 184, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 33.791636, mae: 72.562958, mean_q: 147.772401, mean_eps: 0.408947\n",
      " 13272/20000: episode: 279, duration: 0.602s, episode steps: 110, steps per second: 183, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 23.128362, mae: 72.491643, mean_q: 148.243926, mean_eps: 0.405257\n",
      " 13296/20000: episode: 280, duration: 0.134s, episode steps:  24, steps per second: 179, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 51.267204, mae: 72.411736, mean_q: 147.455465, mean_eps: 0.402243\n",
      " 13500/20000: episode: 281, duration: 1.123s, episode steps: 204, steps per second: 182, episode reward: 204.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 32.454293, mae: 72.608988, mean_q: 148.133999, mean_eps: 0.397112\n",
      " 13829/20000: episode: 282, duration: 1.816s, episode steps: 329, steps per second: 181, episode reward: 329.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 36.918744, mae: 73.137394, mean_q: 149.351246, mean_eps: 0.385120\n",
      " 14141/20000: episode: 283, duration: 1.714s, episode steps: 312, steps per second: 182, episode reward: 312.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 36.414623, mae: 73.814059, mean_q: 150.532664, mean_eps: 0.370697\n",
      " 14555/20000: episode: 284, duration: 2.281s, episode steps: 414, steps per second: 181, episode reward: 414.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.498 [0.000, 1.000],  loss: 33.922994, mae: 73.905221, mean_q: 150.850160, mean_eps: 0.354362\n",
      " 14826/20000: episode: 285, duration: 1.486s, episode steps: 271, steps per second: 182, episode reward: 271.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 30.142897, mae: 74.426562, mean_q: 152.117068, mean_eps: 0.338950\n",
      " 14977/20000: episode: 286, duration: 0.840s, episode steps: 151, steps per second: 180, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 37.540558, mae: 75.409517, mean_q: 153.741394, mean_eps: 0.329455\n",
      " 15214/20000: episode: 287, duration: 1.324s, episode steps: 237, steps per second: 179, episode reward: 237.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 36.823825, mae: 75.821167, mean_q: 154.724571, mean_eps: 0.320725\n",
      " 15604/20000: episode: 288, duration: 2.134s, episode steps: 390, steps per second: 183, episode reward: 390.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 38.871543, mae: 77.372097, mean_q: 157.516092, mean_eps: 0.306617\n",
      " 15852/20000: episode: 289, duration: 1.369s, episode steps: 248, steps per second: 181, episode reward: 248.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 39.467045, mae: 77.633857, mean_q: 157.867048, mean_eps: 0.292262\n",
      " 16352/20000: episode: 290, duration: 2.753s, episode steps: 500, steps per second: 182, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 37.891368, mae: 78.598772, mean_q: 160.042366, mean_eps: 0.275432\n",
      " 16685/20000: episode: 291, duration: 1.869s, episode steps: 333, steps per second: 178, episode reward: 333.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 32.209921, mae: 79.289613, mean_q: 161.622319, mean_eps: 0.256690\n",
      " 16802/20000: episode: 292, duration: 0.656s, episode steps: 117, steps per second: 178, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 26.790918, mae: 79.555076, mean_q: 162.268127, mean_eps: 0.246565\n",
      " 17033/20000: episode: 293, duration: 1.395s, episode steps: 231, steps per second: 166, episode reward: 231.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 40.980468, mae: 80.386144, mean_q: 163.570170, mean_eps: 0.238735\n",
      " 17277/20000: episode: 294, duration: 1.386s, episode steps: 244, steps per second: 176, episode reward: 244.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 34.515176, mae: 81.513870, mean_q: 166.235161, mean_eps: 0.228047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17478/20000: episode: 295, duration: 1.138s, episode steps: 201, steps per second: 177, episode reward: 201.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 37.555728, mae: 82.060479, mean_q: 167.244494, mean_eps: 0.218035\n",
      " 17719/20000: episode: 296, duration: 1.355s, episode steps: 241, steps per second: 178, episode reward: 241.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 39.321987, mae: 81.483270, mean_q: 166.067117, mean_eps: 0.208090\n",
      " 17965/20000: episode: 297, duration: 1.368s, episode steps: 246, steps per second: 180, episode reward: 246.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 41.517352, mae: 81.420026, mean_q: 165.521575, mean_eps: 0.197132\n",
      " 18239/20000: episode: 298, duration: 1.510s, episode steps: 274, steps per second: 181, episode reward: 274.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 38.114417, mae: 81.579115, mean_q: 166.214487, mean_eps: 0.185432\n",
      " 18739/20000: episode: 299, duration: 2.811s, episode steps: 500, steps per second: 178, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 39.118611, mae: 82.032508, mean_q: 167.200927, mean_eps: 0.168017\n",
      " 18969/20000: episode: 300, duration: 1.286s, episode steps: 230, steps per second: 179, episode reward: 230.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 36.898039, mae: 83.284389, mean_q: 169.451409, mean_eps: 0.151592\n",
      " 19238/20000: episode: 301, duration: 1.489s, episode steps: 269, steps per second: 181, episode reward: 269.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 36.154634, mae: 83.340880, mean_q: 169.354588, mean_eps: 0.140365\n",
      " 19738/20000: episode: 302, duration: 2.772s, episode steps: 500, steps per second: 180, episode reward: 500.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 31.602805, mae: 84.521448, mean_q: 171.901101, mean_eps: 0.123062\n",
      "done, took 111.599 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21b77b00ca0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env,nb_steps=20000,visualize=False,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b83958df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights(f'my_weights_cartpole.h5f',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d55e08db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 500.000, steps: 500\n",
      "Episode 2: reward: 401.000, steps: 401\n",
      "Episode 3: reward: 500.000, steps: 500\n",
      "Episode 4: reward: 500.000, steps: 500\n",
      "Episode 5: reward: 368.000, steps: 368\n"
     ]
    }
   ],
   "source": [
    "dqn.test(env,nb_episodes=5,visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52281fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
